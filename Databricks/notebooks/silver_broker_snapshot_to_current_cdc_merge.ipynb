{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "035b471f-6b99-4573-b787-d8f83930dcc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Parameters ###\n",
    "\n",
    "from datetime import datetime               # This imports the datetime class from Python, not Spark.\n",
    "from pyspark.sql import functions as F      # This imports Spark SQL functions (like F.lit, F.sha2, F.current_timestamp) so we can build transformations.\n",
    "\n",
    "storage_account = \"millerinsurancedatalake\"\n",
    "container = \"raw\"\n",
    "entity = \"broker\"\n",
    "pk = \"broker_code\"\n",
    "\n",
    "base = f\"abfss://{container}@{storage_account}.dfs.core.windows.net\"\n",
    "snapshot_root = f\"{base}/landing/onprem/snapshot/{entity}\"\n",
    "silver_schema = \"silver\"\n",
    "silver_table = f\"{silver_schema}.{entity}\"\n",
    "\n",
    "print(f\"base: {base}\")\n",
    "print(f\"snapshot_root: {snapshot_root}\")\n",
    "print(f\"silver_schema: {silver_schema}\")\n",
    "print(f\"silver_table: {silver_table}\")\n",
    "print(f\"pk: {pk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa4a2114-3282-4153-b2bf-494a9868551b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Find latest + previous dt=... files ###\n",
    "\n",
    "import re   # Imports Pythonâ€™s regex library. We use it to extract the date from folder names like dt=2026-01-22.\n",
    "\n",
    "items = dbutils.fs.ls(snapshot_root)        \n",
    "\n",
    "# dbutils.fs.ls lists the contents of a directory as FileInfo objects. Each FileInfo has fields like, path (full path string), name (file name), size (file size in bytes), modifcationTime (modification time in milliseconds since the epoch), etc.\n",
    "\n",
    "dts = []\n",
    "for it in items:\n",
    "    m = re.search(r\"dt=(\\d{4}-\\d{2}-\\d{2})\", it.path)   # regex to extract date from path --> re.search(patth, string)\n",
    "    if m:\n",
    "        dt = m.group(1)\n",
    "        dts.append(dt)\n",
    "\n",
    "dts = sorted(dts)  # just keep sorted list\n",
    "latest_dt = dts[-1]\n",
    "prev_dt = dts[-2] if len(dts) >= 2 else None\n",
    "\n",
    "print(f\"prev_dt: {prev_dt}\")\n",
    "print(f\"latest_dt: {latest_dt}\")\n",
    "\n",
    "latest_path = f\"{snapshot_root}/dt={latest_dt}\"\n",
    "\n",
    "print(f\"latest_path: {latest_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52f65218-8d65-401f-a31e-3a3c6a146d17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Read latest snapshot CSV ###\n",
    "\n",
    "new_df = (spark.read\n",
    "          .format(\"csv\")\n",
    "          .option(\"header\", \"true\")\n",
    "          .option(\"inferSchema\", \"true\")\n",
    "          .load(latest_path))\n",
    "\n",
    "# Add metadata we'll keep in silver\n",
    "new_df = (new_df\n",
    "          .withColumn(\"snapshot_date\", F.lit(latest_dt))\n",
    "          .withColumn(\"ingestion_date\", F.current_timestamp()))\n",
    "\n",
    "display(new_df)\n",
    "display(new_df.select(pk).distinct().count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea16e81c-b837-496d-ac2a-e71d22341a1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Define \"business columns\" + compute row hash ###\n",
    "\n",
    "all_cols = new_df.columns\n",
    "meta_cols = [\"ingestion_date\", \"snapshot_date\"]\n",
    "business_cols = [c for c in all_cols if c not in meta_cols and c != pk]\n",
    "\n",
    "\n",
    "# stable hash: cast to string + coalesce nulls\n",
    "new_df = (new_df\n",
    "          .withColumn(\"row_hash\",\n",
    "            F.sha2(F.concat_ws(\"||\", *[F.coalesce(F.col(c).cast(\"string\"), F.lit(\"\")) for c in business_cols]), 256)))\n",
    "\n",
    "# soft delete fields (for inserts/updates)\n",
    "new_df = (new_df\n",
    "          .withColumn(\"deleted_flag\", F.lit(False))\n",
    "          .withColumn(\"deleted_date\", F.lit(None).cast(\"timestamp\"))\n",
    "          .withColumn(\"last_seen_date\", F.lit(latest_dt)))\n",
    "\n",
    "# display(new_df.select(pk, \"row_hash\", \"deleted\", \"deleted_date\", \"last_seen_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "663af86f-d1d1-4935-91eb-5b96ea133e37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Create schema + first-run table if needed ###\n",
    "\n",
    "silver_db_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/_schemas/{silver_schema}\"\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {silver_schema} MANAGED LOCATION '{silver_db_path}'\")\n",
    "\n",
    "\n",
    "table_exists = spark.catalog.tableExists(silver_table)\n",
    "\n",
    "if not table_exists:\n",
    "    (new_df.select([pk] + business_cols + [\"row_hash\", \"snapshot_date\", \"ingestion_date\", \"deleted_flag\", \"deleted_date\", \"last_seen_date\"])\n",
    "           .write\n",
    "           .format(\"delta\")\n",
    "           .mode(\"overwrite\")\n",
    "           .option(\"path\", silver_db_path)\n",
    "           .saveAsTable(silver_table))\n",
    "    print(f\"Created table: {silver_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e535753e-6e11-4642-b4eb-7ed36534e1a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Build stage ops (I/U/D) vs existing silver\n",
    "\n",
    "silver_df = spark.table(silver_table)\n",
    "\n",
    "# keep only current-state columns we need for comparison\n",
    "silver_active = silver_df.select(pk, \"row_hash\", \"deleted_flag\", \"deleted_date\", \"last_seen_date\")\n",
    "\n",
    "# INSERTS: in new, not in silver\n",
    "ins = (new_df.alias(\"n\")\n",
    "       .join(silver_active.alias(\"s\"), on=pk, how=\"leftanti\")\n",
    "       .withColumn(\"op\", F.lit(\"I\")))\n",
    "\n",
    "# UPDATES: in both and hash changed\n",
    "upd = (new_df.alias(\"n\")\n",
    "       .join(silver_active.alias(\"s\"), on=pk, how=\"inner\")\n",
    "       .where(F.col(\"n.row_hash\") != F.col(\"s.row_hash\"))\n",
    "       .select(\"n.*\")\n",
    "       .withColumn(\"op\", F.lit(\"U\")))\n",
    "\n",
    "# DELETES (missing once): in silver but not in new, and not already deleted\n",
    "missing = (silver_df.alias(\"s\")\n",
    "           .join(new_df.alias(\"n\"), on=pk, how=\"leftanti\")\n",
    "           .where(F.col(\"s.deleted_flag\") == False)\n",
    "           .select(\"s.*\")\n",
    "           .withColumn(\"op\", F.lit(\"D\"))\n",
    "           # we don't have \"new\" values for deletes, but we still need pk + op\n",
    "           .withColumn(\"deleted_date\", F.current_timestamp())\n",
    "           .withColumn(\"deleted_flag\", F.lit(True)))\n",
    "\n",
    "# stage = union of op sets with consistent columns\n",
    "stage_cols = [pk] + business_cols + [\"row_hash\", \"snapshot_date\", \"ingestion_date\", \"deleted_flag\", \"deleted_date\", \"last_seen_date\", \"op\"]\n",
    "stage = (ins.select(stage_cols)\n",
    "         .unionByName(upd.select(stage_cols))\n",
    "         .unionByName(missing.select(stage_cols), allowMissingColumns=True))\n",
    "\n",
    "display(stage.groupBy(\"op\").count())\n",
    "\n",
    "print(missing.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86212de5-9356-41ae-a87d-f289847521fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### MERGE stage into silver\n",
    "\n",
    "stage.createOrReplaceTempView(\"stage_broker\")\n",
    "\n",
    "business_cols = [c.split(\".\")[-1] for c in business_cols]\n",
    "\n",
    "# columns that exist in the silver table\n",
    "update_cols = business_cols + [\n",
    "    \"row_hash\",\n",
    "    \"snapshot_date\",\n",
    "    \"ingestion_date\",\n",
    "    \"deleted_flag\",\n",
    "    \"deleted_date\",\n",
    "    \"last_seen_date\"\n",
    "]\n",
    "\n",
    "set_clause = \",\\n\".join([f\"t.{c} = s.{c}\" for c in update_cols])\n",
    "\n",
    "insert_cols = [pk] + business_cols + [\n",
    "    \"row_hash\",\n",
    "    \"snapshot_date\",\n",
    "    \"ingestion_date\",\n",
    "    \"deleted_flag\",\n",
    "    \"deleted_date\",\n",
    "    \"last_seen_date\"\n",
    "]\n",
    "insert_cols_sql = \", \".join(insert_cols)\n",
    "insert_vals_sql = \", \".join([f\"s.{c}\" for c in insert_cols])\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "MERGE INTO {silver_table} t\n",
    "USING stage_broker s\n",
    "ON t.{pk} = s.{pk}\n",
    "\n",
    "WHEN MATCHED AND s.op = 'U' THEN\n",
    "  UPDATE SET {set_clause}\n",
    "\n",
    "WHEN MATCHED AND s.op = 'D' THEN\n",
    "  UPDATE SET\n",
    "    t.deleted_flag = true,\n",
    "    t.deleted_date = s.deleted_date,\n",
    "    t.last_seen_date = s.last_seen_date\n",
    "\n",
    "WHEN NOT MATCHED AND s.op = 'I' THEN\n",
    "  INSERT ({insert_cols_sql})\n",
    "  VALUES ({insert_vals_sql})\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c929a315-ae5c-4b60-be87-fee9513b5a81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_path = \"abfss://raw@millerinsurancedatalake.dfs.core.windows.net/silver/_schemas/silver\"\n",
    "dbutils.fs.ls(silver_path)  # should show _delta_log + part- files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dec6757b-75e8-4b48-a59f-abe8935197db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(silver_path)\n",
    "df.show(20, truncate=False)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8dbba85-c399-4491-b7f1-81fce46aaafd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## 1 Post-run data validation / sanity check\n",
    "\n",
    "df.createOrReplaceTempView(\"broker_current_path\")\n",
    "\n",
    "spark.sql(\"SELECT COUNT(*) AS total_rows FROM broker_current_path\").show()\n",
    "spark.sql(\"SELECT COUNT(*) AS deleted_rows FROM broker_current_path WHERE deleted_flag = true\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c32929c9-1f8b-485c-82ee-811c3c0bf06e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## latest snapshot count (adjust path)\n",
    "original_path = \"abfss://raw@millerinsurancedatalake.dfs.core.windows.net/landing/onprem/snapshot/broker/dt=2026-01-21\"\n",
    "snap_path = \"abfss://raw@millerinsurancedatalake.dfs.core.windows.net/landing/onprem/snapshot/broker/dt=2026-01-22\"\n",
    "\n",
    "original = spark.read.option(\"header\", True).csv(original_path)\n",
    "snap = spark.read.option(\"header\", True).csv(snap_path)\n",
    "\n",
    "print(\"original:\", original.count())\n",
    "print(\"snapshot:\", snap.count())\n",
    "print(\"silver:\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce5dc4bd-6414-4510-a1cf-9ba36203b31b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## DQ summary\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "pk = \"broker_code\"\n",
    "\n",
    "# --- metrics ---\n",
    "total_rows   = df.count()\n",
    "deleted_rows = df.filter(F.col(\"deleted_flag\") == True).count()\n",
    "pk_nulls     = df.filter(F.col(pk).isNull()).count()\n",
    "\n",
    "dupes_df = (\n",
    "    df.filter(F.col(\"deleted_flag\") == False)\n",
    "      .groupBy(pk).count()\n",
    "      .filter(F.col(\"count\") > 1)\n",
    "      .orderBy(F.col(\"count\").desc())\n",
    ")\n",
    "\n",
    "dupe_rows = dupes_df.count()\n",
    "\n",
    "# --- compact summary line ---\n",
    "print(\n",
    "    f\"Rows={total_rows} | Deleted={deleted_rows} | PK nulls={pk_nulls} | PK dupes={dupe_rows}\"\n",
    ")\n",
    "\n",
    "# --- only show tables when there's something to investigate ---\n",
    "if pk_nulls > 0:\n",
    "    display(df.filter(F.col(pk).isNull()).select(pk).limit(50))\n",
    "\n",
    "if dupe_rows > 0:\n",
    "    display(dupes_df.limit(50))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_broker_snapshot_to_current_cdc_merge",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
